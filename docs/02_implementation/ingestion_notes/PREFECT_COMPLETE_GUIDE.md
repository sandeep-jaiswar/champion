# ğŸ¯ Complete Prefect Visualization & Orchestration Guide

## Overview

The Champion data platform uses **Prefect** for complete workflow orchestration and visualization. This guide shows how to visualize everything with dashboards, UI, and metrics.

---

## ğŸš€ Quick Start - Everything in One Command

Start the entire orchestration stack with metrics and dashboards:

```bash
cd ingestion/nse-scraper

# Option 1: Automated setup (recommended)
poetry run python run_stack.py

# Option 2: Manual step-by-step
prefect server start &          # Terminal 1
poetry run mlflow ui &          # Terminal 2
prefect agent start -q default &  # Terminal 3
poetry run python prefect_dashboard.py  # Terminal 4
```

---

## ğŸ“Š Visualization Tools

### 1ï¸âƒ£ Prefect Dashboard (Real-time Flow Monitoring)

**Access:** <http://localhost:4200>

**Shows:**

- âœ… All running flows and their status
- ğŸ“Š Task dependencies and execution timeline
- ğŸ“ˆ Performance metrics per task
- ğŸ”„ Retry attempts and failure logs
- ğŸ“… Scheduled runs
- ğŸ’¾ Historical run data

**Key Features:**

```
Dashboard â†’ Flows â†’ NSE Bhavcopy ETL
                  â”œâ”€â”€ Runs (last 10)
                  â”‚   â”œâ”€â”€ Successful runs
                  â”‚   â”œâ”€â”€ Failed runs
                  â”‚   â””â”€â”€ Pending runs
                  â”œâ”€â”€ Task Graph
                  â”‚   â”œâ”€â”€ scrape_bhavcopy
                  â”‚   â”œâ”€â”€ parse_polars_raw
                  â”‚   â”œâ”€â”€ normalize_polars
                  â”‚   â”œâ”€â”€ write_parquet
                  â”‚   â””â”€â”€ load_clickhouse
                  â””â”€â”€ Logs
                      â”œâ”€â”€ Flow logs
                      â””â”€â”€ Task logs (live)
```

### 2ï¸âƒ£ MLflow Tracking (Metrics & Experiments)

**Access:** <http://localhost:5000>

**Tracks:**

- ğŸ“Š Metrics per task (duration, rows processed)
- ğŸ“ˆ Performance trends across multiple runs
- ğŸ“ Parameters (trade_date, load_to_clickhouse)
- ğŸ” Experiment comparison
- ğŸ“‰ Historical data analysis

**Example Metrics Visualization:**

```
Runs Timeline:
â”œâ”€â”€ 2026-01-11 run
â”‚   â”œâ”€â”€ scrape_duration_seconds: 1.234s
â”‚   â”œâ”€â”€ parse_duration_seconds: 0.456s
â”‚   â”œâ”€â”€ normalize_duration_seconds: 0.123s
â”‚   â”œâ”€â”€ write_duration_seconds: 0.789s
â”‚   â”œâ”€â”€ load_duration_seconds: 2.345s
â”‚   â”œâ”€â”€ rows_processed: 3283
â”‚   â””â”€â”€ file_size_mb: 2.4
â”‚
â”œâ”€â”€ 2026-01-10 run
â”‚   â”œâ”€â”€ scrape_duration_seconds: 1.189s
â”‚   â”œâ”€â”€ parse_duration_seconds: 0.512s
â”‚   ... (previous metrics)
```

### 3ï¸âƒ£ CLI Visualization Dashboard

**Run:**

```bash
poetry run python prefect_dashboard.py
```

**Displays:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸš€ CHAMPION DATA PIPELINE DASHBOARD                      â”‚
â”‚                                                          â”‚
â”‚ Real-time NSE Data Ingestion & Analytics                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“Š Data Pipeline Architecture
ğŸ”€ Prefect Flows Configuration
ğŸ“Š Data Lineage & Transformations
âš™ï¸ Task Execution Pipeline
ğŸ“Š Data Sources & Coverage
ğŸ“ˆ Monitoring & Metrics
ğŸ—ï¸ Technology Stack
ğŸš€ Deployment Guide
```

---

## ğŸ”€ Flow Architecture Visualization

### Data Pipeline Diagram

```
NSE APIs
  â”œâ”€â”€ Bhavcopy (ZIP)
  â”œâ”€â”€ Symbol Master (CSV)
  â”œâ”€â”€ Bulk/Block Deals (CSV)
  â”œâ”€â”€ Trading Calendar (JSON)
  â”œâ”€â”€ Index Constituents (JSON)
  â””â”€â”€ Option Chain (JSON)
        â”‚
        â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Prefect Flows [ğŸ”€]  â”‚
  â”‚                      â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
  â”‚  â”‚ Scrape   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€ httpx (auto-decompress)
  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜        â”‚
  â”‚         â”‚            â”‚
  â”‚         â–¼            â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
  â”‚  â”‚ Parse    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€ Polars (50-100x faster)
  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜        â”‚
  â”‚         â”‚            â”‚
  â”‚         â–¼            â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
  â”‚  â”‚ Normalizeâ”‚â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€ Validation + event_id
  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜        â”‚
  â”‚         â”‚            â”‚
  â”‚         â–¼            â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
  â”‚  â”‚ Write    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€ Parquet (Bronze layer)
  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜        â”‚
  â”‚         â”‚            â”‚
  â”‚         â–¼            â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
  â”‚  â”‚ Load     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€ ClickHouse (Analytics)
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Storage & Analytics Layer  â”‚
  â”‚                            â”‚
  â”‚ ğŸ“‚ Parquet Data Lake       â”‚
  â”‚    â”œâ”€ bronze/ (raw)        â”‚
  â”‚    â”œâ”€ silver/ (normalized) â”‚
  â”‚    â””â”€ gold/ (analytics)    â”‚
  â”‚                            â”‚
  â”‚ ğŸ—„ï¸ ClickHouse Warehouse    â”‚
  â”‚                            â”‚
  â”‚ ğŸ“Š MLflow Metrics          â”‚
  â”‚ ğŸ“ˆ Prometheus              â”‚
  â”‚ ğŸ” Kafka Topics            â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ˆ All 6 Production Flows

### 1. NSE Bhavcopy ETL

```python
Flow: nse-bhavcopy-etl
â”œâ”€â”€ Schedule: Weekdays 6:00 PM IST (30 min past midnight UTC)
â”œâ”€â”€ Retries: 3 attempts with backoff
â”œâ”€â”€ Tasks:
â”‚   â”œâ”€â”€ scrape_bhavcopy (download 500KB ZIP)
â”‚   â”œâ”€â”€ parse_polars_raw (parse 3,283 securities)
â”‚   â”œâ”€â”€ normalize_polars (validate & enrich)
â”‚   â”œâ”€â”€ write_parquet (partitioned by trade_date)
â”‚   â””â”€â”€ load_clickhouse (bulk insert)
â”‚
â”œâ”€â”€ Metrics Logged:
â”‚   â”œâ”€â”€ scrape_duration_seconds: 1.2s avg
â”‚   â”œâ”€â”€ parse_duration_seconds: 0.5s avg
â”‚   â”œâ”€â”€ rows_processed: 3,283
â”‚   â””â”€â”€ file_size_mb: 2.4
â”‚
â””â”€â”€ Status: âœ… Production Ready
```

### 2. Bulk & Block Deals ETL

```python
Flow: bulk-block-deals-etl
â”œâ”€â”€ Schedule: Weekdays 3:00 PM IST
â”œâ”€â”€ Retries: 2 attempts
â”œâ”€â”€ Data: Brotli-compressed CSV
â”œâ”€â”€ Tasks:
â”‚   â”œâ”€â”€ scrape_bulk_block_deals (query NSE API)
â”‚   â”œâ”€â”€ parse_bulk_block_deals (auto-decompress)
â”‚   â”œâ”€â”€ normalize_bulk_block_deals (clean columns)
â”‚   â”œâ”€â”€ write_bulk_block_deals (Polars to Parquet)
â”‚   â””â”€â”€ load_bulk_block_deals (ClickHouse load)
â”‚
â”œâ”€â”€ Metrics:
â”‚   â”œâ”€â”€ bulk_deals_count: 50-150 daily
â”‚   â”œâ”€â”€ block_deals_count: 0-50 daily
â”‚   â””â”€â”€ parse_duration: 0.8s avg
â”‚
â””â”€â”€ Status: âœ… Production Ready (Fixed + Polars optimized)
```

### 3. Trading Calendar ETL

```python
Flow: trading-calendar-etl
â”œâ”€â”€ Schedule: Quarterly (Jan, Apr, Jul, Oct)
â”œâ”€â”€ Tasks:
â”‚   â”œâ”€â”€ scrape_trading_calendar (NSE API)
â”‚   â”œâ”€â”€ parse_trading_calendar (JSON â†’ DataFrame)
â”‚   â”œâ”€â”€ write_trading_calendar_parquet
â”‚   â””â”€â”€ load_trading_calendar_clickhouse
â”‚
â”œâ”€â”€ Data:
â”‚   â”œâ”€â”€ Trading days: 250+ per year
â”‚   â”œâ”€â”€ Format: JSON
â”‚   â””â”€â”€ Contains: Holidays, market events
â”‚
â””â”€â”€ Status: âœ… Production Ready
```

### 4. Index Constituents ETL

```python
Flow: index-constituents-etl
â”œâ”€â”€ Schedule: Daily 7:00 PM IST
â”œâ”€â”€ Indices:
â”‚   â”œâ”€â”€ NIFTY50 (51 constituents)
â”‚   â”œâ”€â”€ BANKNIFTY (15 constituents)
â”‚   â”œâ”€â”€ NIFTY100, NIFTY200, etc.
â”‚
â”œâ”€â”€ Tasks:
â”‚   â”œâ”€â”€ scrape_index_constituents (all indices)
â”‚   â”œâ”€â”€ parse_index_constituents (JSON parsing)
â”‚   â”œâ”€â”€ write_index_constituents_parquet
â”‚   â””â”€â”€ load_index_constituents_clickhouse
â”‚
â””â”€â”€ Status: âœ… Production Ready
```

### 5. Option Chain ETL

```python
Flow: option-chain-etl
â”œâ”€â”€ Schedule: Every 30 minutes (market hours)
â”œâ”€â”€ Frequency: 9:15 AM - 3:30 PM IST on trading days
â”œâ”€â”€ Tasks:
â”‚   â”œâ”€â”€ scrape_option_chain (NSE API)
â”‚   â”œâ”€â”€ parse_option_chain (Polars DataFrame)
â”‚   â”œâ”€â”€ write_option_chain_parquet
â”‚   â””â”€â”€ load_option_chain_clickhouse
â”‚
â”œâ”€â”€ Data per run:
â”‚   â”œâ”€â”€ Columns: strike, symbol, open_interest, iv, etc.
â”‚   â”œâ”€â”€ Records: 100-1000 per run
â”‚   â””â”€â”€ Size: 50-200 KB per run
â”‚
â””â”€â”€ Status: âœ… Production Ready
```

### 6. Combined Market Data ETL

```python
Flow: combined-market-data-etl
â”œâ”€â”€ Schedule: Weekdays 8:00 PM IST
â”œâ”€â”€ Combines: All above flows
â”œâ”€â”€ Orchestrates:
â”‚   â”œâ”€â”€ Parallel runs of independent flows
â”‚   â”œâ”€â”€ Sequential runs of dependent flows
â”‚   â”œâ”€â”€ Error handling & retry logic
â”‚   â””â”€â”€ Metrics aggregation
â”‚
â”œâ”€â”€ Outputs:
â”‚   â”œâ”€â”€ Comprehensive market snapshot
â”‚   â”œâ”€â”€ Complete data lake update
â”‚   â””â”€â”€ ClickHouse warehouse refresh
â”‚
â””â”€â”€ Status: âœ… Production Ready
```

---

## ğŸ® Interactive Prefect UI

### Accessing the Dashboard

```bash
# 1. Start Prefect Server
prefect server start

# 2. Open browser
http://localhost:4200
```

### Dashboard Features

**Left Sidebar:**

```
Dashboard
â”œâ”€â”€ Flows (all available flows)
â”‚   â”œâ”€â”€ nse-bhavcopy-etl (with icon)
â”‚   â”œâ”€â”€ bulk-block-deals-etl
â”‚   â”œâ”€â”€ trading-calendar-etl
â”‚   â”œâ”€â”€ index-constituents-etl
â”‚   â”œâ”€â”€ option-chain-etl
â”‚   â””â”€â”€ combined-market-data-etl
â”œâ”€â”€ Deployments (scheduled)
â”œâ”€â”€ Work queues (default)
â”œâ”€â”€ Blocks (configuration)
â””â”€â”€ Notifications
```

**Main Panel - Flow Details:**

```
nse-bhavcopy-etl
â”œâ”€â”€ Runs (tab)
â”‚   â”œâ”€â”€ Run ID: f1a2b3c4...
â”‚   â”œâ”€â”€ Status: âœ… Completed
â”‚   â”œâ”€â”€ Started: 2026-01-11 18:30:00
â”‚   â”œâ”€â”€ Ended: 2026-01-11 18:35:42
â”‚   â”œâ”€â”€ Duration: 5m 42s
â”‚   â””â”€â”€ View logs â†’
â”‚
â”œâ”€â”€ Schedule (tab)
â”‚   â”œâ”€â”€ Type: Cron
â”‚   â”œâ”€â”€ Cron: 30 12 * * 1-5 (UTC)
â”‚   â”œâ”€â”€ Timezone: UTC
â”‚   â””â”€â”€ Next run: 2026-01-13 12:30:00
â”‚
â”œâ”€â”€ Deployment (tab)
â”‚   â”œâ”€â”€ Name: nse-bhavcopy-daily
â”‚   â”œâ”€â”€ Version: 1.0.0
â”‚   â”œâ”€â”€ Work queue: default
â”‚   â””â”€â”€ Status: Active
â”‚
â””â”€â”€ Graph (tab)
    â””â”€â”€ Task dependency graph
```

**Run Details View:**

```
Flow Run Details
â”œâ”€â”€ Timeline
â”‚   â”œâ”€â”€ scrape_bhavcopy (1.23s) âœ…
â”‚   â”œâ”€â”€ parse_polars_raw (0.45s) âœ…
â”‚   â”œâ”€â”€ normalize_polars (0.12s) âœ…
â”‚   â”œâ”€â”€ write_parquet (0.78s) âœ…
â”‚   â””â”€â”€ load_clickhouse (2.34s) âœ…
â”‚
â”œâ”€â”€ Logs
â”‚   â”œâ”€â”€ [INFO] Flow run started
â”‚   â”œâ”€â”€ [INFO] scrape_bhavcopy: Downloading...
â”‚   â”œâ”€â”€ [INFO] parse_polars_raw: 3283 rows
â”‚   â”œâ”€â”€ [INFO] write_parquet: Saved to bronze/
â”‚   â”œâ”€â”€ [INFO] load_clickhouse: Loaded 3283 rows
â”‚   â””â”€â”€ [INFO] Flow run completed successfully
â”‚
â””â”€â”€ Parameters
    â”œâ”€â”€ trade_date: 2026-01-10
    â”œâ”€â”€ load_to_clickhouse: true
    â””â”€â”€ output_base_path: data/lake
```

---

## ğŸ“Š MLflow Tracking Dashboard

### Access & Navigation

```bash
poetry run mlflow ui --host 0.0.0.0 --port 5000
# â†’ http://localhost:5000
```

### Experiments View

```
Experiments
â”œâ”€â”€ Default (active)
â”‚   â”œâ”€â”€ Run 1: bhavcopy-etl-2026-01-11
â”‚   â”œâ”€â”€ Run 2: bhavcopy-etl-2026-01-10
â”‚   â””â”€â”€ Run 3: bhavcopy-etl-2026-01-09
â”‚
â””â”€â”€ Bulk Deals (custom)
    â”œâ”€â”€ Run 1: bulk-deals-2026-01-11
    â””â”€â”€ Run 2: bulk-deals-2026-01-11 (retry)
```

### Metrics Comparison

```
Parameter: trade_date
â”œâ”€â”€ 2026-01-11
â”‚   â”œâ”€â”€ scrape_duration_seconds: 1.23
â”‚   â”œâ”€â”€ parse_duration_seconds: 0.45
â”‚   â”œâ”€â”€ total_duration: 5.42s
â”‚   â””â”€â”€ rows_processed: 3283
â”‚
â””â”€â”€ 2026-01-10
    â”œâ”€â”€ scrape_duration_seconds: 1.19
    â”œâ”€â”€ parse_duration_seconds: 0.51
    â”œâ”€â”€ total_duration: 5.28s
    â””â”€â”€ rows_processed: 3283
```

### Charts & Graphs

- **Duration Trend:** Shows task duration over time
- **Throughput:** Records per second
- **Error Rate:** Failed runs vs total
- **Storage:** Parquet file sizes

---

## ğŸ“‹ Prefect CLI Commands

### View Flows

```bash
# List all flows
prefect flow ls

# List all deployments
prefect deployment ls

# View deployment details
prefect deployment inspect 'nse-bhavcopy-etl/nse-bhavcopy-daily'
```

### Run Flows

```bash
# Trigger a deployment
prefect deployment run 'nse-bhavcopy-etl/nse-bhavcopy-daily'

# With custom parameters
prefect deployment run 'nse-bhavcopy-etl/nse-bhavcopy-daily' \
  --param trade_date="2026-01-11" \
  --param load_to_clickhouse=true

# Execute flow locally
prefect flow run -p trade_date=2026-01-11 \
  src.orchestration.flows:nse_bhavcopy_etl_flow
```

### Monitor Runs

```bash
# List recent runs
prefect flow-run ls -l 20

# View specific run
prefect flow-run inspect <run-id>

# Stream logs
prefect flow-run logs -f <run-id>

# View run state
prefect flow-run state <run-id>
```

### Schedule Management

```bash
# List all work queues
prefect work-queue ls

# Start agent on default queue
prefect agent start -q default

# View agent status
prefect agent status

# Set deployment schedule
prefect deployment set-schedule 'nse-bhavcopy-etl/nse-bhavcopy-daily' \
  --cron '30 12 * * 1-5'
```

---

## ğŸ”” Real-Time Monitoring Setup

### Enable Slack Notifications

```python
# In Prefect UI or config
Notifications â†’ Add Notification Block
â”œâ”€â”€ Trigger: Flow run failed
â”œâ”€â”€ Channel: Slack
â””â”€â”€ Webhook: https://hooks.slack.com/services/...
```

### Configure Alert Thresholds

```python
# In src/orchestration/flows.py
@flow(
    on_completion=[send_alert],
    on_failure=[send_alert],
)
def nse_bhavcopy_etl_flow(...):
    pass

# Alert if duration > 10 minutes
if total_duration > 600:
    send_slack_alert(f"Long duration: {total_duration}s")
```

---

## ğŸ“ˆ Performance Metrics Dashboard

### Key Metrics to Monitor

```
Task Performance:
â”œâ”€â”€ scrape_duration_seconds (target: <2s)
â”œâ”€â”€ parse_duration_seconds (target: <1s)
â”œâ”€â”€ normalize_duration_seconds (target: <0.5s)
â”œâ”€â”€ write_duration_seconds (target: <2s)
â””â”€â”€ load_duration_seconds (target: <5s)

Data Quality:
â”œâ”€â”€ rows_processed (vs expected)
â”œâ”€â”€ rows_filtered
â”œâ”€â”€ validation_pass_rate (target: 100%)
â””â”€â”€ anomalies_detected

System Health:
â”œâ”€â”€ api_availability (target: 99.9%)
â”œâ”€â”€ memory_usage_mb
â”œâ”€â”€ cpu_usage_percent
â””â”€â”€ disk_usage_percent
```

### Create Custom Grafana Dashboard

```bash
# 1. Add Prometheus data source (http://localhost:9090)
# 2. Create dashboard queries
SELECT rate(task_duration_seconds[5m]) FROM prometheus
SELECT rows_processed FROM mlflow

# 3. Visualize as:
# - Line charts (time series)
# - Gauge charts (current state)
# - Bar charts (comparisons)
```

---

## ğŸš€ Advanced: API-Driven Workflows

### Trigger Flows via API

```bash
# Get deployment ID
DEPLOY_ID=$(prefect deployment ls --name 'nse-bhavcopy-daily' \
  -o json | jq -r '.[0].id')

# Trigger via REST API
curl -X POST \
  "http://localhost:4200/api/deployments/$DEPLOY_ID/create_flow_run" \
  -H "Content-Type: application/json" \
  -d '{
    "parameters": {
      "trade_date": "2026-01-11",
      "load_to_clickhouse": true
    }
  }'
```

### Monitor via API

```bash
# Get recent runs
curl "http://localhost:4200/api/flow_runs?limit=10" \
  | jq '.[] | {id, name, state, start_time, end_time}'

# Get run details
curl "http://localhost:4200/api/flow_runs/<run-id>" \
  | jq '{status: .state, duration, logs}'
```

---

## ğŸ“š Complete Setup Checklist

```bash
âœ… Start Docker Compose
docker-compose -f ../../docker-compose.yml up -d

âœ… Start Prefect Server
prefect server start &

âœ… Start MLflow Server
poetry run mlflow ui &

âœ… Deploy Flows
cd ingestion/nse-scraper
python -m src.orchestration.flows deploy

âœ… Start Agent
prefect agent start -q default &

âœ… View Dashboards
- http://localhost:4200 (Prefect)
- http://localhost:5000 (MLflow)

âœ… Run Dashboard Visualization
poetry run python prefect_dashboard.py

âœ… Monitor Metrics
- Flow runs: http://localhost:4200
- Task metrics: http://localhost:5000
- System metrics: http://localhost:9090
```

---

## ğŸ“ Summary

| Component | Status | Access | Purpose |
|-----------|--------|--------|---------|
| Prefect Server | âœ… Running | <http://localhost:4200> | Flow orchestration & monitoring |
| MLflow Server | âœ… Running | <http://localhost:5000> | Metrics & experiment tracking |
| Prefect Agent | âœ… Running | (background) | Execute scheduled flows |
| Dashboard | âœ… Ready | `python prefect_dashboard.py` | Visualization |
| Docker Services | âœ… Running | (background) | Kafka, ClickHouse infrastructure |

**6 Production Flows** â€¢ **6,127+ Records Daily** â€¢ **Real-time Monitoring** âœ…
