version: '3.8'

# Network definitions for service isolation
networks:
  frontend:
    driver: bridge
  backend:
    driver: bridge
  monitoring:
    driver: bridge

# Volume definitions for persistent data
volumes:
  clickhouse_data:
    driver: local
  clickhouse_logs:
    driver: local
  mlflow_data:
    driver: local
  prefect_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local

services:
  # =============================================================================
  # Champion Application Service
  # =============================================================================
  champion:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: champion-app
    hostname: champion-app
    networks:
      - backend
      - monitoring
    ports:
      - "8080:8080"  # Health check endpoint
      - "9090:9090"  # Metrics endpoint
    volumes:
      - ./data:/data
      - ./logs:/app/logs
    environment:
      - ENVIRONMENT=production
      - LOG_LEVEL=INFO
      - PYTHONUNBUFFERED=1
    env_file:
      - .env
    depends_on:
      clickhouse:
        condition: service_healthy
      mlflow:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped
    # Resource limits
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '0.5'
          memory: 1G

  # =============================================================================
  # ClickHouse Database Service
  # =============================================================================
  clickhouse:
    image: clickhouse/clickhouse-server:24.1
    container_name: champion-clickhouse
    hostname: clickhouse
    networks:
      - backend
      - monitoring
    ports:
      - "8123:8123"  # HTTP interface
      - "9000:9000"  # Native protocol
    volumes:
      - clickhouse_data:/var/lib/clickhouse
      - clickhouse_logs:/var/log/clickhouse-server
      - ./warehouse/clickhouse/init:/docker-entrypoint-initdb.d:ro
    environment:
      CLICKHOUSE_DB: champion_market
      CLICKHOUSE_USER: champion_user
      CLICKHOUSE_PASSWORD: champion_pass
      CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT: 1
    env_file:
      - .env
    ulimits:
      nofile:
        soft: 262144
        hard: 262144
    healthcheck:
      test: ["CMD", "clickhouse-client", "--user", "champion_user", "--password", "champion_pass", "--query", "SELECT 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    # Resource limits
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 8G
        reservations:
          cpus: '1.0'
          memory: 4G

  # =============================================================================
  # MLflow Tracking Service
  # =============================================================================
  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.9.2
    container_name: champion-mlflow
    hostname: mlflow
    networks:
      - backend
      - monitoring
    ports:
      - "5000:5000"
    volumes:
      - mlflow_data:/mlflow
    environment:
      MLFLOW_BACKEND_STORE_URI: sqlite:///mlflow/mlflow.db
      MLFLOW_DEFAULT_ARTIFACT_ROOT: /mlflow/artifacts
    command: >
      mlflow server
      --backend-store-uri sqlite:///mlflow/mlflow.db
      --default-artifact-root /mlflow/artifacts
      --host 0.0.0.0
      --port 5000
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: unless-stopped
    # Resource limits
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.25'
          memory: 512M

  # =============================================================================
  # Prefect Orchestration Service
  # =============================================================================
  prefect-server:
    image: prefecthq/prefect:2.20-python3.11
    container_name: champion-prefect
    hostname: prefect
    networks:
      - backend
      - monitoring
    ports:
      - "4200:4200"  # Prefect UI
    volumes:
      - prefect_data:/root/.prefect
    environment:
      PREFECT_SERVER_API_HOST: 0.0.0.0
      PREFECT_API_URL: http://prefect:4200/api
      PREFECT_UI_URL: http://localhost:4200
    command: prefect server start --host 0.0.0.0
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4200/api/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s
    restart: unless-stopped
    # Resource limits
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.25'
          memory: 512M

  # =============================================================================
  # Prometheus Monitoring Service
  # =============================================================================
  prometheus:
    image: prom/prometheus:v2.48.0
    container_name: champion-prometheus
    hostname: prometheus
    networks:
      - monitoring
    ports:
      - "9091:9090"  # Using 9091 to avoid conflict with champion metrics
    volumes:
      - prometheus_data:/prometheus
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    restart: unless-stopped
    # Resource limits
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.25'
          memory: 512M

  # =============================================================================
  # Grafana Visualization Service
  # =============================================================================
  grafana:
    image: grafana/grafana:10.2.2
    container_name: champion-grafana
    hostname: grafana
    networks:
      - monitoring
      - frontend
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards:ro
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: admin
      GF_INSTALL_PLUGINS: ""
      GF_SERVER_ROOT_URL: http://localhost:3000
      GF_AUTH_ANONYMOUS_ENABLED: "false"
    env_file:
      - .env
    depends_on:
      prometheus:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s
    restart: unless-stopped
    # Resource limits
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 1G
        reservations:
          cpus: '0.1'
          memory: 256M
