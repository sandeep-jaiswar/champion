#!/usr/bin/env python3
"""Simple ETL runner that doesn't require MLflow."""

import os
import sys
from datetime import date, timedelta
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent))

print("ğŸš€ Starting Champion NSE Data Pipeline")
print("=" * 60)

# Step 1: Scrape
print("\nğŸ“¥ Step 1: Scraping NSE Bhavcopy Data...")
from src.scrapers.bhavcopy import BhavcopyScraper
from src.utils.logger import configure_logging, get_logger

configure_logging()
logger = get_logger(__name__)

target_date = date.today() - timedelta(days=1)
scraper = BhavcopyScraper()

try:
    csv_file = scraper.scrape(target_date, dry_run=False)
    print(f"âœ… Downloaded: {csv_file}")
except Exception as e:
    print(f"âŒ Scrape failed: {e}")
    sys.exit(1)

# Step 2: Parse
print("\nğŸ“Š Step 2: Parsing with Polars...")
from src.parsers.polars_bhavcopy_parser import PolarsBhavcopyParser

parser = PolarsBhavcopyParser()
try:
    raw_df = parser.parse_raw_csv(csv_file)
    print(f"âœ… Parsed {len(raw_df)} rows")
except Exception as e:
    print(f"âŒ Parse failed: {e}")
    sys.exit(1)

# Step 3: Normalize
print("\nğŸ”§ Step 3: Normalizing data...")
try:
    normalized_df = parser.normalize(raw_df)
    print(f"âœ… Normalized {len(normalized_df)} rows")
except Exception as e:
    print(f"âŒ Normalize failed: {e}")
    sys.exit(1)

# Step 4: Write Parquet
print("\nğŸ’¾ Step 4: Writing to Parquet...")
try:
    output_file = parser.write_parquet(
        df=normalized_df,
        trade_date=target_date,
        base_path=Path("../../data/lake")
    )
    print(f"âœ… Written to: {output_file}")
except Exception as e:
    print(f"âŒ Write failed: {e}")
    sys.exit(1)

# Summary
print("\n" + "=" * 60)
print("ğŸ‰ ETL Pipeline Completed Successfully!")
print("=" * 60)
print(f"\nğŸ“… Trade Date: {target_date}")
print(f"ğŸ“ˆ Rows Processed: {len(normalized_df)}")
print(f"ğŸ“ Output: {output_file}")
print(f"ğŸ’¾ File Size: {output_file.stat().st_size / (1024*1024):.2f} MB")

# Show sample data
print("\nğŸ“Š Sample Data:")
print(normalized_df.head(5))
print("\nâœ¨ Next steps:")
print("  1. View data: ls -lh data/lake/normalized/equity_ohlc/**/*.parquet")
print("  2. Load to ClickHouse: poetry run python warehouse/loader/batch_loader.py")
print("  3. Compute features: poetry run python src/features/demo_features.py")
