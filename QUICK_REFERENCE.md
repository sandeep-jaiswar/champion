# Champion Architecture - Quick Reference

## File Structure

```
src/champion/
â”œâ”€â”€ core/                          # Foundation (new)
â”‚   â”œâ”€â”€ __init__.py               # Public API exports
â”‚   â”œâ”€â”€ config.py                 # Unified AppConfig
â”‚   â”œâ”€â”€ di.py                     # Dependency injection container
â”‚   â”œâ”€â”€ errors.py                 # Exception hierarchy
â”‚   â”œâ”€â”€ interfaces.py             # Abstract base classes
â”‚   â””â”€â”€ logging.py                # Structured logging
â”‚
â”œâ”€â”€ scrapers/                      # Data ingestion
â”‚   â”œâ”€â”€ adapters.py               # EquityScraper, ReferenceDataScraper
â”‚   â”œâ”€â”€ nse/
â”‚   â”‚   â”œâ”€â”€ bhavcopy.py           # NSE OHLC
â”‚   â”‚   â”œâ”€â”€ equity_list.py        # Symbol master
â”‚   â”‚   â”œâ”€â”€ corporate_actions.py
â”‚   â”‚   â””â”€â”€ option_chain.py
â”‚   â””â”€â”€ bse/                       # BSE scrapers
â”‚
â”œâ”€â”€ storage/                       # File-based lake
â”‚   â”œâ”€â”€ adapters.py               # ParquetDataSink, CSVDataSource, etc
â”‚   â”œâ”€â”€ parquet_io.py             # I/O utilities
â”‚   â””â”€â”€ retention.py              # Cleanup policies
â”‚
â”œâ”€â”€ warehouse/                     # OLAP warehouse
â”‚   â”œâ”€â”€ adapters.py               # WarehouseSink, ClickHouseSink
â”‚   â”œâ”€â”€ clickhouse/
â”‚   â”‚   â”œâ”€â”€ batch_loader.py       # Merged from /warehouse/loader
â”‚   â”‚   â””â”€â”€ models/               # DDL definitions
â”‚   â””â”€â”€ models/                    # Data models
â”‚
â”œâ”€â”€ validation/                    # Quality checks
â”‚   â”œâ”€â”€ validator.py              # Main validator (merged from /validation)
â”‚   â””â”€â”€ demo.py
â”‚
â”œâ”€â”€ features/                      # Analytics & indicators
â”‚   â”œâ”€â”€ indicators.py             # SMA, EMA, RSI, etc
â”‚   â”œâ”€â”€ portfolio.py              # Portfolio metrics
â”‚   â””â”€â”€ risk.py                   # Risk calculations
â”‚
â”œâ”€â”€ corporate_actions/             # Dividend/split handling
â”‚   â”œâ”€â”€ models.py
â”‚   â””â”€â”€ processor.py
â”‚
â”œâ”€â”€ orchestration/                 # Workflows
â”‚   â”œâ”€â”€ config.py                 # (backward compat, redirects to core)
â”‚   â”œâ”€â”€ flows/
â”‚   â”‚   â”œâ”€â”€ flows.py              # Main ETL flows
â”‚   â”‚   â””â”€â”€ trading_calendar_flow.py
â”‚   â””â”€â”€ tasks/                    # Atomic Prefect tasks
â”‚
â”œâ”€â”€ utils/                         # Shared utilities
â”‚   â”œâ”€â”€ logger.py                 # (use core.logging instead)
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ __init__.py                    # Public API + docs
â”œâ”€â”€ config.py                      # Backward compat re-exports
â””â”€â”€ cli.py                         # Unified CLI commands
```

## Core Module Exports

### Configuration

```python
from champion.core import get_config, AppConfig, Environment

config = get_config()
if config.is_prod():
    ...
```

### Logging

```python
from champion.core import get_logger, configure_logging, get_request_id

logger = get_logger(__name__)
logger.info("Processing", request_id=get_request_id())
```

### Errors

```python
from champion.core import (
    ChampionError,
    ValidationError,
    DataError,
    IntegrationError,
    ConfigError,
)

try:
    data = validate(df)
except ValidationError as e:
    print(e.recovery_hint)
```

### Dependency Injection

```python
from champion.core import get_container, Container, ServiceLocator

container = get_container()
container.register(DataSource, lambda c: ParquetDataSource())
source = container.resolve(DataSource)
```

### Interfaces

```python
from champion.core import (
    DataSource,      # Read data
    DataSink,        # Write data
    Transformer,     # Process data
    Validator,       # Check quality
    Scraper,         # Extract data
    Repository,      # Data access
    CacheBackend,    # Caching
    Observer,        # Events
    DataContext,     # Metadata
)
```

## Domain Module Exports

### Scrapers

```python
from champion.scrapers import (
    EquityScraper,              # Abstract base
    ReferenceDataScraper,       # Abstract base
    ScraperWithRetry,           # Decorator
)
from champion.scrapers.nse import NSEBhavcopyScraper
```

### Storage

```python
from champion.storage import (
    ParquetDataSource,
    ParquetDataSink,
    CSVDataSource,
    CSVDataSink,
)
```

### Warehouse

```python
from champion.warehouse import (
    WarehouseSink,              # Abstract
    ClickHouseSink,             # Concrete
)
```

### Features

```python
from champion.features import (
    compute_sma,
    compute_ema,
    compute_rsi,
    compute_features,
)
```

### Validation

```python
from champion.validation import (
    validate_data,
    quarantine_failed_records,
)
```

## Common Patterns

### Pattern 1: Simple Data Processing

```python
from champion.core import get_logger, get_config
from champion.scrapers import EquityScraper
from champion.storage import ParquetDataSink

logger = get_logger(__name__)
config = get_config()

# Scrape
scraper = YourScraper()
data = scraper.scrape_date(date(2024, 1, 15))

# Store
sink = ParquetDataSink(config.storage.data_dir)
sink.connect()
sink.write(data, file_path="raw/equity.parquet")
sink.disconnect()

logger.info("Complete", rows=len(data))
```

### Pattern 2: Using Dependency Injection

```python
from champion.core import DataSink, get_logger

class DataProcessor:
    def __init__(self, sink: DataSink):
        self.sink = sink
        self.logger = get_logger(__name__)
    
    def process(self, data):
        result = self.transform(data)
        return self.sink.write(result)
    
    def transform(self, data):
        # Your logic
        return data

# Usage
from champion.warehouse import ClickHouseSink
processor = DataProcessor(ClickHouseSink())
processor.process(df)
```

### Pattern 3: Error Handling

```python
from champion.core import (
    get_logger,
    ValidationError,
    IntegrationError,
)

logger = get_logger(__name__)

try:
    data = validator.validate(df)
    if data.has_errors:
        raise ValidationError(
            "Data validation failed",
            recovery_hint="Check input format"
        )
    warehouse.write(data)
except ValidationError as e:
    logger.error("Validation", error=e.code, hint=e.recovery_hint)
except IntegrationError as e:
    if e.retryable:
        logger.warning("Retrying", service=e.service)
        # retry logic
    else:
        logger.error("Fatal", error=e.message)
```

### Pattern 4: Configuration

```python
from champion.core import get_config

config = get_config()

# Access nested configs
clickhouse_host = config.clickhouse.host
nse_url = config.nse.bhavcopy_url
storage_dir = config.storage.data_dir
log_level = config.observability.logging.level

# Environment checks
if config.is_prod():
    batch_size = 1000000
else:
    batch_size = 10000
```

### Pattern 5: Registering Custom Implementations

```python
from champion.core import get_container
from champion.core import DataSink

class CustomSink(DataSink):
    def connect(self): pass
    def write(self, data, **kwargs): pass
    def disconnect(self): pass

container = get_container()
container.register(DataSink, lambda c: CustomSink(), lifetime="singleton")

# Later
sink = container.resolve(DataSink)  # Gets your CustomSink
```

## Command Cheat Sheet

```bash
# Show configuration
poetry run champion show-config

# Run ETL flows
poetry run champion etl-ohlc                                    # Run for yesterday
poetry run champion etl-ohlc --start-date 2024-01-01 --end-date 2024-01-31
poetry run champion etl-macro
poetry run champion etl-index --index NIFTY50
poetry run champion etl-trading-calendar

# Warehouse operations
poetry run champion warehouse load --table raw_ohlc --source data/

# Testing
poetry run pytest tests/                                        # All tests
poetry run pytest tests/unit/                                  # Unit only
poetry run pytest tests/integration/                           # Integration only
poetry run pytest tests/ -v                                    # Verbose
poetry run pytest tests/ --cov=champion                        # With coverage

# Code quality
poetry run black .                                             # Format
poetry run ruff check .                                        # Lint
poetry run mypy src/                                           # Type check
```

## Debugging Tips

### 1. Check Configuration

```bash
poetry run champion show-config | grep -i clickhouse
```

### 2. Enable Debug Logging

```bash
LOG_LEVEL=DEBUG poetry run champion etl-ohlc
```

### 3. List Available Services

```python
from champion.core import get_container
container = get_container()
print(container._services.keys())
```

### 4. Test Data Source

```python
from champion.storage import ParquetDataSource
source = ParquetDataSource("data/raw")
source.connect()
df = source.read("equity_ohlc.parquet")
print(df.shape)
```

### 5. Check Imports

```bash
cd src && python -c "import champion; print(champion.__version__)"
```

## Troubleshooting

| Problem | Solution |
|---------|----------|
| `ModuleNotFoundError` | Check import path, use `from champion.core import ...` |
| `ConfigError` | Check `.env` file, verify env variables |
| `ValidationError` | Check data format against schema |
| `IntegrationError` | Check ClickHouse/Kafka connectivity |
| `Cannot resolve service` | Register in DI container first |
| `Circular import` | Use `from champion.core import Interface` not concrete class |

## Performance Tips

1. **Use Polars** for data processing (vectorized, zero-copy)
2. **Batch operations** when writing to warehouse
3. **Stream large files** using `read_batch()`
4. **Enable compression** in ParquetDataSink
5. **Use connection pooling** for ClickHouse
6. **Cache reference data** with CacheBackend

## File Locations

| What | Where |
|------|-------|
| Documentation | `docs/ARCHITECTURE.md`, `docs/MIGRATION.md` |
| Tests | `tests/unit/`, `tests/integration/` |
| Fixtures | `tests/conftest.py` |
| Configuration template | `.env` (create from scratch) |
| Raw data | `data/raw/` |
| Processed data | `data/lake/` |
| Logs | `logs/` |
| ML tracking | `mlruns/` |
| Schemas | `schemas/parquet/`, `schemas/json/` |

## Key Documents

- ðŸ“– **ARCHITECTURE.md** - Complete architecture guide
- ðŸ”„ **MIGRATION.md** - Step-by-step migration guide
- ðŸ“‹ **ARCHITECTURE_TRANSFORMATION.md** - What was done
- ðŸ”§ **This file** - Quick reference

---

**Champion: From Fragmented to Unified Architecture** âœ¨

*Built on clean architecture principles for maintainability, scalability, and developer experience*
